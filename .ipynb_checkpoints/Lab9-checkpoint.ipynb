{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x111b3dd90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.load('citibike.csv', format='csv', header=True, inferScheme=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cartodb_id', 'string'),\n",
       " ('the_geom', 'string'),\n",
       " ('tripduration', 'string'),\n",
       " ('starttime', 'string'),\n",
       " ('stoptime', 'string')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "import csv\n",
    "\n",
    "def parseCSV(idx, part):\n",
    "    if idx == 0:\n",
    "        part.next()\n",
    "    for p in csv.reader(part):\n",
    "        yield Row(tripduration=int(p[2]),\n",
    "                  startingtime=p[3],\n",
    "                   start_station_time=p[6])\n",
    "                  \n",
    "rows = sc.textFile('citibike.csv').mapPartitionsWithIndex(parseCSV)\n",
    "df = sqlContext.createDataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tripduration', 'int'),\n",
       " ('starttime', 'string'),\n",
       " ('start_station_time', 'string')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "import csv\n",
    "\n",
    "def parseCSV(idx, part):\n",
    "    if idx == 0:\n",
    "        part.next()\n",
    "    for p in csv.reader(part):\n",
    "        yield (int(p[2]), p[3], p[6])\n",
    "                  \n",
    "rows = sc.textFile('citibike.csv').mapPartitionsWithIndex(parseCSV)\n",
    "schema = StructType([StructField('tripduration', IntegerType()),\n",
    "                    StructField('starttime', StringType()),\n",
    "                    StructField('start_station_time', StringType())])\n",
    "df = sqlContext.createDataFrame(rows, schema)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|   c|\n",
      "+----+\n",
      "|2537|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "\n",
    "df.agg(sf.approx_count_distinct(df.tripduration).alias('c')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.registerTempTable('citibike')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(tripduration=60, starttime=u'2015-02-02 21:39:00+00', start_station_time=u'W 31 St & 7 Ave'),\n",
       " Row(tripduration=60, starttime=u'2015-02-04 07:52:00+00', start_station_time=u'W 29 St & 9 Ave'),\n",
       " Row(tripduration=60, starttime=u'2015-02-03 08:07:00+00', start_station_time=u'Mott St & Prince St'),\n",
       " Row(tripduration=60, starttime=u'2015-02-02 17:14:00+00', start_station_time=u'5 Ave & E 29 St'),\n",
       " Row(tripduration=60, starttime=u'2015-02-03 18:01:00+00', start_station_time=u'Grand St & Havemeyer St')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql('select * from citibike order by tripduration limit 5').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(percentile(tripduration, CAST(0.5 AS DOUBLE))=529.0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql('select percentile(tripduration, 0.5) from citibike').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAT_FN = 'SAT_Results.csv'\n",
    "HSD_FN = 'DOE_High_School_Directory_2014-2015.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSchools = spark.read.load(HSD_FN, format='csv', header=True, inferSchema=True) \\\n",
    "                      .na.drop(subset=['boro'])\n",
    "    \n",
    "dfSchools = dfSchools.filter(dfSchools['total_students']>500)\n",
    "dfSchools = dfSchools.select('dbn','boro')\n",
    "dfSchools.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[SAT Math Avg. Score: int]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfScores = sqlContext.read.load(SAT_FN, format='csv', header=True, inferSchema=True)\n",
    "dfScores.select(dfScores['`SAT Math Avg. Score`'].cast('int'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DBN', 'string'),\n",
       " ('SCHOOL NAME', 'string'),\n",
       " ('Num of SAT Test Takers', 'string'),\n",
       " ('SAT Critical Reading Avg. Score', 'string'),\n",
       " ('SAT Math Avg. Score', 'string'),\n",
       " ('SAT Writing Avg. Score', 'string')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfScores.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-------+\n",
      "|   DBN|sum_score|ntakers|\n",
      "+------+---------+-------+\n",
      "|02M047|     6400|     16|\n",
      "|21K410|   207575|    475|\n",
      "|30Q301|    43120|     98|\n",
      "|17K382|    22066|     59|\n",
      "|18K637|    13335|     35|\n",
      "|32K403|    18300|     50|\n",
      "|09X365|    18306|     54|\n",
      "|11X270|    22064|     56|\n",
      "|05M367|    12078|     33|\n",
      "|14K404|    24276|     68|\n",
      "|30Q575|    66420|    135|\n",
      "|13K336|     3366|      9|\n",
      "|04M635|    17712|     48|\n",
      "|24Q264|    40406|     89|\n",
      "|17K408|    19494|     57|\n",
      "|19K618|    22260|     60|\n",
      "|27Q309|    13644|     36|\n",
      "|32K552|    24388|     67|\n",
      "|13K499|    26208|     72|\n",
      "|07X600|    30400|     76|\n",
      "+------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfMScores = dfScores.select('DBN',\n",
    "                           dfScores['`SAT Math Avg. Score`'].cast('int').alias('score'),\n",
    "                           dfScores['Num of SAT Test Takers'].cast('int').alias('ntakers')) \\\n",
    "                            .na.drop()\n",
    "        \n",
    "dfMScores = dfMScores.select('DBN',\n",
    "                            (dfMScores.score*dfMScores.ntakers).alias('sum_score'),\n",
    "                            'ntakers')\n",
    "        \n",
    "dfMScores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+------------+\n",
      "|         boro|sum(sum_score)|sum(ntakers)|\n",
      "+-------------+--------------+------------+\n",
      "|       Queens|       5190534|       10942|\n",
      "|     Brooklyn|       4544126|        9322|\n",
      "|Staten Island|       1406967|        2944|\n",
      "|    Manhattan|       3206992|        6228|\n",
      "|        Bronx|       1619364|        3444|\n",
      "+-------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = dfSchools.join(dfMScores, dfSchools.dbn==dfMScores.DBN, how='inner')\n",
    "df = df.groupBy('boro').sum('sum_score','ntakers')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+\n",
      "|         boro|avg|\n",
      "+-------------+---+\n",
      "|       Queens|474|\n",
      "|     Brooklyn|487|\n",
      "|Staten Island|477|\n",
      "|    Manhattan|514|\n",
      "|        Bronx|470|\n",
      "+-------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('avg',(df[1]/df[2]).cast('int')).select('boro','avg').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[88] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfTweets = sqlContext.read.load('twitter_1k.jsonl',format='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|           hashtag|count|\n",
      "+------------------+-----+\n",
      "|USSaluteouttuesday|   33|\n",
      "|               NYC|    5|\n",
      "| privatepassSports|    4|\n",
      "|               nyc|    4|\n",
      "|         superbowl|    3|\n",
      "|         redcarpet|    3|\n",
      "|               Job|    3|\n",
      "|   SuperBowlXLVIII|    3|\n",
      "|              espn|    3|\n",
      "|       RobinThicke|    3|\n",
      "|    henleyvaporium|    2|\n",
      "|             ncaaw|    2|\n",
      "|      ESPNTHEPARTY|    2|\n",
      "|             facts|    2|\n",
      "|         SuperBowl|    2|\n",
      "|           NewYork|    2|\n",
      "|              soho|    2|\n",
      "|                IT|    2|\n",
      "|               Raw|    2|\n",
      "|              Jobs|    2|\n",
      "+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashtags = dfTweets.select('entities.hashtags.text')\n",
    "hashtags = hashtags.filter(sf.size('text')>0)\n",
    "hashtags = hashtags.select(sf.explode('text').alias('hashtag'))\n",
    "hashtags = hashtags.groupBy('hashtag').count().orderBy('count',ascending=False)\n",
    "hashtags.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [python2]",
   "language": "python",
   "name": "Python [python2]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
